{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "dffa4f43",
   "metadata": {},
   "source": [
    "# Facial Recognition"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3b1b4b94",
   "metadata": {},
   "source": [
    "### Import Standard and Tensorflow Libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "aab05a7f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Importing standard libraries\n",
    "import cv2\n",
    "import os\n",
    "import random\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# Importing Tensorflow dependencies\n",
    "from tensorflow.keras.models import Model\n",
    "from tensorflow.keras.layers import Layer, Conv2D, Dense, MaxPooling2D, Input, Flatten\n",
    "import tensorflow as tf"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f3d6e7ff",
   "metadata": {},
   "source": [
    "### Set Memory Growth to aviod OOM Errors"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5ee0784d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Avoid OOM errors by setting GPU Memory Growth\n",
    "\n",
    "gpus = tf.config.experimental.list_physical_devices('GPU')\n",
    "logical_gpus = tf.config.list_logical_devices('GPU')\n",
    "for gpu in gpus:\n",
    "    tf.config.set_memory_growth(gpu, True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1f46248d",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(len(gpus), gpus)\n",
    "print(len(logical_gpus), logical_gpus)\n",
    "print(tf.test.is_gpu_available)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8c95f90f",
   "metadata": {},
   "source": [
    "### Create Folder Structures"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f10a1a1f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Setup Paths (Directories)\n",
    "\n",
    "ANC_PATH = os.path.join('data', 'anchor2')\n",
    "POS_PATH = os.path.join('data', 'positive2')\n",
    "NEG_PATH = os.path.join('data', 'negative')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c58bff2c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# # Using List to create the directories -> Done Once\n",
    "# list = [ANC_PATH, NEG_PATH, POS_PATH]\n",
    "# for ele in list:\n",
    "#     print(ele)\n",
    "#     os.makedirs(ele)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cc800ea3",
   "metadata": {},
   "source": [
    "### Untar LFW Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2d4832f4",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Uncompress the Tar GZ Labelled Faces in the Wild Dataset\n",
    "\n",
    "# !tar -xf lfw.tgz"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "29c49b83",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Transfer the images in every sub-folder in the lfw folder to NEG_PATH\n",
    "\n",
    "# for directory in os.listdir('lfw'):\n",
    "#     for file in os.listdir(os.path.join('lfw', directory)):\n",
    "#         EX_PATH = os.path.join('lfw', directory, file)\n",
    "#         NEW_PATH = os.path.join(NEG_PATH, file)\n",
    "#         os.replace(EX_PATH, NEW_PATH)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3be67903",
   "metadata": {},
   "source": [
    "### Collect Positive and Anchor Classes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8eeee775",
   "metadata": {},
   "outputs": [],
   "source": [
    "# We are going to make sure the images we collect from webcam are of dimension/resolution: \n",
    "# (250 px X 250 px, i.e., of same resolution as images in the LFW dataset)\n",
    "# This makes the data processing step easier."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b0c29e38",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Importing uuid to generate unique image names\n",
    "import uuid"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f56295a2",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Establish a connection to the Webcam -> To collect Anchor and Positive images\n",
    "cap = cv2.VideoCapture(0)\n",
    "while cap.isOpened():\n",
    "    ret, frame = cap.read()\n",
    "    \n",
    "    frame = frame[120:120+250, 200:200+250, :] # Changed the dimension of captured frame to 250X250\n",
    "    \n",
    "    # Collect Anchors\n",
    "    if cv2.waitKey(1) & 0XFF == ord('a'):\n",
    "        imgname = os.path.join(ANC_PATH, '{}.jpg'.format(uuid.uuid1())) # Appends unique identifer to .jpg and joins to ANC_PATH\n",
    "        cv2.imwrite(imgname, frame)\n",
    "    \n",
    "    # Collect Positives\n",
    "    if cv2.waitKey(1) & 0XFF == ord('p'):\n",
    "        imgname = os.path.join(POS_PATH, '{}.jpg'.format(uuid.uuid1()))\n",
    "        cv2.imwrite(imgname, frame)\n",
    "    \n",
    "    # Show image back to screen\n",
    "    cv2.imshow('Image Collector Window', frame)\n",
    "\n",
    "    if cv2.waitKey(1) & 0XFF == ord('q'):\n",
    "        break\n",
    "\n",
    "# Release the Webcam\n",
    "cap.release()\n",
    "\n",
    "# Close the image show frame\n",
    "cv2.destroyAllWindows()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7a95c767",
   "metadata": {},
   "source": [
    "### Geting Image Directories"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "39c099c3",
   "metadata": {},
   "outputs": [],
   "source": [
    "anchor = tf.data.Dataset.list_files(ANC_PATH + '\\*.jpg').shuffle(buffer_size = 1024).take(300)\n",
    "positive = tf.data.Dataset.list_files(POS_PATH + '\\*.jpg').shuffle(buffer_size = 1024).take(300)\n",
    "negative = tf.data.Dataset.list_files(NEG_PATH + '\\*.jpg').shuffle(buffer_size = 1024).take(300)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c66de6f3",
   "metadata": {},
   "source": [
    "### Preprocessing - Scaling And Resizing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4f117fad",
   "metadata": {},
   "outputs": [],
   "source": [
    "def preprocess(file_path):\n",
    "    byte_img = tf.io.read_file(file_path)\n",
    "    img = tf.io.decode_jpeg(byte_img)\n",
    "    img = tf.image.resize(img, (100,100)) # Resizing from 255x255 to 100x100 (number of pixel values for an image)\n",
    "    img = img / 255.0 # Rescaling from 0 to 255 to 0 to 1 (on image pixel values)\n",
    "    return img"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "02b70af3",
   "metadata": {},
   "source": [
    "### Creating Labelled Datasets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ed59e4ec",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Using the tf.data.Dataset.from_tensor_slices to create a td.data.Dataset object having a sequence of tensor of 1's (each 1 is a 0-dimensional tensor (scalar value))\n",
    "positives = tf.data.Dataset.zip((anchor, positive, tf.data.Dataset.from_tensor_slices(tf.ones(len(anchor)))))\n",
    "negatives = tf.data.Dataset.zip((anchor, negative, tf.data.Dataset.from_tensor_slices(tf.zeros(len(anchor)))))\n",
    "data = positives.concatenate(negatives)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b31f2026",
   "metadata": {},
   "source": [
    "### Build Train and Test Partition"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "03ce4dfe",
   "metadata": {},
   "outputs": [],
   "source": [
    "def preprocess_twin(input_img, validation_img, label):\n",
    "    return(preprocess(input_img), preprocess(validation_img), label)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9d7c0e15",
   "metadata": {},
   "outputs": [],
   "source": [
    "# build DataLoader Pipeline\n",
    "data = data.map(preprocess_twin) # Preprocessing all the images in data using map funcion\n",
    "data = data.cache() # Caches dataset in memory -> does not need to read from disk multiple times (improves performance)\n",
    "data = data.shuffle(buffer_size = 1024) # size of Buffer used to shuffle dataset -> larger buffer: more random shuffles"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ecb33235",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Training Partition\n",
    "train_data = data.take(round(len(data)*0.7))\n",
    "train_data = train_data.batch(16) # data will be passed for training as batches of 16 images\n",
    "train_data = train_data.prefetch(8) # This starts preprocessing next set of images so that we don't bottle-nexk out network\n",
    "# Now iterating the training the training_data using as_numpy_iterator will give us a batch of 16 images rather than a single image when we use the next() function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4ba3bf10",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Testing Partition\n",
    "test_data = data.skip(round(len(data)*0.7)) # This data was taken for training. Data cannot be repeated in training and testing partition\n",
    "test_data = test_data.take(round(len(data)*0.3))\n",
    "test_data = test_data.batch(16)\n",
    "test_data = test_data.prefetch(8)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "242dc3fb",
   "metadata": {},
   "source": [
    "### Building an embedding Layer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b5a275a9",
   "metadata": {},
   "outputs": [],
   "source": [
    "def make_embedding():\n",
    "    \n",
    "    # Input Layer - Base Class\n",
    "    inp = Input(shape=(100,100,3), name='Input_Image')\n",
    "    \n",
    "    # First Block\n",
    "    c1 = Conv2D(64, (10,10), strides = (1,1), activation='relu')(inp)\n",
    "    # There are 64 Filters/Kernels. Each Kernel is of size 10x10. Strides(How far filters move across image) = 1x1.\n",
    "    # Activation function is ReLU. inp is passed to the layer which essentially connects the input layer to the c1 Convolution Layer\n",
    "    m1 = MaxPooling2D(64, (2,2), padding='same')(c1)\n",
    "    \n",
    "    # Second Block\n",
    "    c2 = Conv2D(128, (7, 7), activation='relu')(m1)\n",
    "    m2 = MaxPooling2D(64, (2,2), padding='same')(c2)\n",
    "    \n",
    "    # Third Block\n",
    "    c3 = Conv2D(128, (4,4), activation='relu')(m2)\n",
    "    m3 = MaxPooling2D(64, (2,2), padding='same')(c3)\n",
    "    \n",
    "    # Final Block\n",
    "    c4 = Conv2D(256, (4,4), activation='relu')(m3)\n",
    "    f1 = Flatten()(c4)\n",
    "    d1 = Dense(4096, activation='sigmoid')(f1)\n",
    "    \n",
    "    return Model(inputs=[inp], outputs=[d1], name = 'Embedding')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "51faa662",
   "metadata": {},
   "outputs": [],
   "source": [
    "embedding = make_embedding()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0fb9c41a",
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "embedding.summary()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e340e39b",
   "metadata": {},
   "source": [
    "### Building L1 Distance Layer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9bae307e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Siamese L1 Distance class\n",
    "class L1Dist(Layer):\n",
    "    # Init Method - inheritance\n",
    "    def __init__(self, **kwargs):\n",
    "        super().__init__()\n",
    "\n",
    "    # Similarity calculation\n",
    "    def call(self, input_embedding, validation_embedding):\n",
    "        return tf.math.abs(input_embedding - validation_embedding)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "af55c63f",
   "metadata": {},
   "source": [
    "### Make Siamese Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "469958ec",
   "metadata": {},
   "outputs": [],
   "source": [
    "def make_siamese_model():\n",
    "    \n",
    "    # 2 images are input into the network at once \n",
    "    \n",
    "    # Anchor image input to the network\n",
    "    input_image = Input(shape=(100,100,3), name = 'Input_img')\n",
    "    \n",
    "    # Validation image input to the network\n",
    "    validation_image = Input(shape=(100,100,3), name = 'Validation_img')\n",
    "    \n",
    "    # Combine Siamese Distance Components\n",
    "    siamese_layer = L1Dist() # Creating an instance of L1Dist class\n",
    "    siamese_layer._name = 'distance'\n",
    "    # Using the call function to calculate the distance between the feature vector of input_img and validation_img\n",
    "    distances = siamese_layer(embedding(input_image), embedding(validation_image)) # Using the embedding model\n",
    "    \n",
    "    # Classification Layer\n",
    "    classifier = Dense(1, activation = 'sigmoid')(distances)\n",
    "    \n",
    "    return Model(inputs=[input_image, validation_image], outputs=[classifier], name = 'SiameseNetwork')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9de33eb8",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Using the function\n",
    "Siamese_Model = make_siamese_model()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "67b07732",
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "Siamese_Model.summary()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a294b7b0",
   "metadata": {},
   "source": [
    "### Setup Loss Function and Optimizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b378a297",
   "metadata": {},
   "outputs": [],
   "source": [
    "binary_cross_loss = tf.losses.BinaryCrossentropy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9c9d1ecb",
   "metadata": {},
   "outputs": [],
   "source": [
    "opt = tf.keras.optimizers.Adam(learning_rate = 1e-4) # 0.0001"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d0350a92",
   "metadata": {},
   "source": [
    "### Establish Checkpoints"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8c0bb0c9",
   "metadata": {},
   "outputs": [],
   "source": [
    "checkpoint_dir = '\\.training_checkpoints'\n",
    "checkpoint_prefix = os.path.join(checkpoint_dir, 'ckpt')\n",
    "checkpoint = tf.train.Checkpoint(opt = opt, siamese_model = Siamese_Model)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0f149d52",
   "metadata": {},
   "source": [
    "### Build Train Step Function - will be used on 1 Batch of data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "49105d53",
   "metadata": {},
   "outputs": [],
   "source": [
    "@tf.function\n",
    "def train_step(batch):\n",
    "    \n",
    "    # Recording all of our operations in tape\n",
    "    with tf.GradientTape() as tape:\n",
    "\n",
    "        # Get anchor and positive/negative image\n",
    "        X = batch[:2] \n",
    "        \n",
    "        # Get label\n",
    "        y = batch[2]\n",
    "    \n",
    "        # Forward Pass\n",
    "        yhat = Siamese_Model(X, training=True)\n",
    "        \n",
    "        # Calculate Loss\n",
    "        loss = binary_cross_loss(y, yhat)\n",
    "    \n",
    "    print(loss)\n",
    "    \n",
    "    # Calculate Gradients\n",
    "    grad = tape.gradient(loss, Siamese_Model.trainable_variables)\n",
    "    \n",
    "    # Calculate updated weights and apply to Siamese Model\n",
    "    opt.apply_gradients(zip(grad, Siamese_Model.trainable_variables))\n",
    "    \n",
    "    # Return loss\n",
    "    return loss"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ed32efe8",
   "metadata": {},
   "source": [
    "### Build Training Loop"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a0abf960",
   "metadata": {},
   "outputs": [],
   "source": [
    "def train(data, EPOCHS):\n",
    "    # Loop through each Epoch\n",
    "    for epoch in range(1, EPOCHS+1):\n",
    "        print('\\nEpoch {}/{}'.format(epoch, EPOCHS))\n",
    "        progbar = tf.keras.utils.Progbar(len(data))\n",
    "        \n",
    "        r = Recall()\n",
    "        p = Precision()\n",
    "        \n",
    "        # Loop through each batch\n",
    "        for idx, batch in enumerate(data):\n",
    "            # Run Train step\n",
    "            loss = train_step(batch)\n",
    "            yhat = Siamese_Model.predict(batch[:2])\n",
    "            r.update_state(batch[2], yhat)\n",
    "            p.update_state(batch[2], yhat)\n",
    "            progbar.update(idx+1)\n",
    "        print(loss.numpy())\n",
    "        # save Checkpoints\n",
    "        if epoch%10 == 0:\n",
    "            checkpoint.save(file_prefix = checkpoint_prefix)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "000908f9",
   "metadata": {},
   "source": [
    "### Train the Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6623b150",
   "metadata": {},
   "outputs": [],
   "source": [
    "EPOCHS = 50"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7c9e73f8",
   "metadata": {},
   "outputs": [],
   "source": [
    "# train(train_data, EPOCHS) # This task done by Google Colab (T4 GPU Runtime) as the training process takes a long time using CPU"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "11696dd0",
   "metadata": {},
   "source": [
    "### Save Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dba5da37",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Save Weights\n",
    "# Siamese_Model.save('/content/gdrive/MyDrive/SiameseModel2.h5') # Model saved from Google Colab after training and then loaded in Jupyter Notebook to be able to access the Webcam easily"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "af8653f3",
   "metadata": {},
   "source": [
    "### Reload Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a44d661c",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# Reload Model\n",
    "model = tf.keras.models.load_model('SiameseModel2.h5', custom_objects={'L1Dist': L1Dist, 'BinaryCrossentropy': tf.losses.BinaryCrossentropy})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7d5df04d",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "model.summary()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2bc95d93",
   "metadata": {},
   "source": [
    "# #6 Evaluate Model"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d5149c23",
   "metadata": {},
   "source": [
    "### Import Metrics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4df2b8fa",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Importing metrtic calculations\n",
    "from tensorflow.keras.metrics import Precision, Recall"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f9e1d3ce",
   "metadata": {},
   "outputs": [],
   "source": [
    "test_input, test_vali, y_true = test_data.as_numpy_iterator().next()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ec3f36c8",
   "metadata": {},
   "source": [
    "### Make Predictions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6b1f87d6",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "yhat = model.predict([test_input, test_vali])\n",
    "yhat # Predicted output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "714c7afc",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# Post Processing the Results\n",
    "[1 if prediction > 0.5 else 0 for prediction in yhat]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c2d3dcb8",
   "metadata": {},
   "source": [
    "### Calculate Metrics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a58db4a9",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Creating metric object\n",
    "r = Recall()\n",
    "p = Precision()\n",
    "\n",
    "# Calculating Recall Value\n",
    "r.update_state(y_true, yhat)\n",
    "# Calculating Precision Value\n",
    "p.update_state(y_true, yhat)\n",
    "\n",
    "print(\"This is for a Single Batch in the test_data dataset:\")\n",
    "# Return Recall Result\n",
    "print(\"Recall: \", r.result().numpy())\n",
    "# Return Precision Result\n",
    "print(\"Precision: \", p.result().numpy())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3fca6ef9",
   "metadata": {},
   "source": [
    "### Visualize Results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "65427ec6",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Set plot Size\n",
    "plt.figure(figsize=(5,8))\n",
    "\n",
    "# Creating figure with 1 row and 2 columns\n",
    "\n",
    "# Selecting first Subplot\n",
    "plt.subplot(1,2,1)\n",
    "plt.imshow(test_input[2])\n",
    "\n",
    "# Selecting second subplot\n",
    "plt.subplot(1,2,2)\n",
    "plt.imshow(test_vali[2])\n",
    "\n",
    "# Renders cleanly\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f1dc5cc3",
   "metadata": {},
   "source": [
    "### Verification Function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4618fbc8",
   "metadata": {},
   "outputs": [],
   "source": [
    "def verify(model, detection_threshold, verification_threshold):\n",
    "  # Build results array\n",
    "    results = []\n",
    "    for image in os.listdir(os.path.join('application_data', 'verification_images2')):\n",
    "        input_img = preprocess(os.path.join('application_data', 'input_image', 'input_image.jpg'))\n",
    "        validation_img = preprocess(os.path.join('application_data', 'verification_images2', image))\n",
    "\n",
    "        # Make predictions\n",
    "        result = model.predict(list(np.expand_dims([input_img, validation_img], axis = 1)))\n",
    "        results.append(result)\n",
    "\n",
    "    # Detection Threshold: Metric above which a prediction is considered positive\n",
    "    detection = np.sum(np.array(results) > detection_threshold)\n",
    "\n",
    "    # Verification Threshold: Proportion of positive predictions / total positive samples\n",
    "    verification = detection/len(os.listdir(os.path.join('application_data', 'verification_images2')))\n",
    "    verified = verification > verification_threshold # returns true or false\n",
    "\n",
    "    return results, verified"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "77707d5a",
   "metadata": {},
   "source": [
    "### OpenCV Real Time Verification"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b7ed98f6",
   "metadata": {},
   "outputs": [],
   "source": [
    "cap = cv2.VideoCapture(0)\n",
    "while cap.isOpened():\n",
    "    ret, frame = cap.read()\n",
    "\n",
    "    frame = frame[120:120+250, 200:200+250, :] # Changed the dimension of captured frame to 250X250\n",
    "\n",
    "    cv2.imshow('Verification', frame)\n",
    "\n",
    "    # Verification Trigger\n",
    "    if cv2.waitKey(10) & 0xFF == ord('v'):\n",
    "        # Save Input Image to application_data/input_image folder\n",
    "        cv2.imwrite(os.path.join('application_data', 'input_image', 'input_image.jpg'), frame)\n",
    "\n",
    "        # Run Verification\n",
    "        results, verified = verify(model, 0.5, 0.5)\n",
    "        print(verified)\n",
    "\n",
    "    if cv2.waitKey(10) & 0xFF == ord('q'):\n",
    "        break\n",
    "\n",
    "cap.release()\n",
    "cv2.destroyAllWindows()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2e8d91fc",
   "metadata": {},
   "outputs": [],
   "source": [
    "np.sum(np.squeeze(results) > 0.5)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
